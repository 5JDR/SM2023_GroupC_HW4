---
title: "GroupC_HW4"
output: html_document
date: "2023-12-22"
---

# FSDS - Chapter 4
## Exercise 4.24
Refer to the vegetarian survey result in Exercise 4.6, with $n = 25$ and no vegetarians.
(a) Find the Bayesian estimate of π using a beta prior distribution with $\alpha = \beta$ equal 
(i) 0.5,
(ii) 1.0, 
(iii) 10.0. 
Explain how the choice of prior distribution affects the posterior mean estimate.
(b) If you were planning how to take a larger survey from the same population, explain how
you can use the posterior results of the previous survey with $n = 25$ based on the prior
with $\alpha = \beta = 1$ to form the prior distribution to use with the new survey results.

**Solution**

```{r}
n = 25
vegetarian = rep(0, n)
```

```{r}
alpha <- beta <- 0.5
curve(dbeta(x, alpha, beta), ylab="Density", xlim=c(-0.01,0.5), ylim=c(0, 200))
curve(dbeta(x, alpha + sum(vegetarian), beta + n - sum(vegetarian)), add=TRUE, col="red")
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "red"), lty=1)
```

```{r}
alpha <- beta <- 1
curve(dbeta(x, alpha, beta), ylab="Density", xlim=c(-0.01,0.5), ylim=c(0, 30))
curve(dbeta(x, alpha + sum(vegetarian), beta + n - sum(vegetarian)), add=TRUE, col="red")
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "red"), lty=1)
```

```{r}
alpha <- beta <- 10
curve(dbeta(x, alpha, beta), ylab="Density", xlim=c(-0.01,1), ylim=c(0, 10))
curve(dbeta(x, alpha + sum(vegetarian), beta + n - sum(vegetarian)), add=TRUE, col="red")
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "red"), lty=1)
```
The posterior mean will approach the prior mean, so using the value 0.5 and 1 the mean will be near to zero and, using the value 10, the posterior mean will be between zero and the prior mean. We can notice that the posterior mean is 0.2 so the data mean weights more than the prior mean.

####################
######TO CHECK######
####################

```{r}
x <- seq(0, 1, 100)
values <- dbeta(x, 1 + sum(vegetarian), 1 + n - sum(vegetarian))
maximum <- max(values)
p <- x[values == maximum] # Equal to zero

# Always zero
new <- rbinom(10, 1, p)
new
```

## Exercise 4.62
For the bootstrap method, explain the similarity and difference between the true sampling
distribution of $\hat{\theta}$ and the empirically-generated bootstrap distribution in terms of its center
and its spread.

**Solution**

The true sampling distribution is the distribution of the statistic obtained if we infinitely sample from the population. The bootstrap distribution is the distribution of the statistic obtained if we sample from the dataset. For the CLT the centers of both the distributions will be similar, but the spread of the bootstrap distribution will be smaller than the true sampling distribution because based on a limited amount of elements. Therefore, the bootstrap distribution will be affect from the original dataset bias.


# FSDS - Chapter 8
## Exercise 8.24
As the basis for a statistical analysis, explain the difference between a statistical model and an
algorithm. For each, explain the extent to which it can deal with summarizing effects, inferential
statistics, and prediction.

**Solution**

With the statistical models we can summarize the effects of a population, like means and variances, and make inference about the population parameters based on the sample, while the algorithms are more suited to make predictions based on the input data, but they usually can't make inference about the population parameters or the distribution that generated them.

# LAB
Suppose you receive $n = 15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i∼Exponential(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

1. $\pi(\lambda)=Beta(4,2)$
2. $\pi(\lambda)=Normal(1,2)$
3. $\pi(\lambda)=Gamma(4,2)$

Now, compute your posterior as $\pi(\lambda|y) \propto L(\lambda;y)\pi(\lambda)$
for the selected prior. If your first choice was correct, you will be able to compute it analytically.

**Solution**


# ISLR - Chapter 7 - Exercises 7.9
## Exercise 1
It was mentioned in the chapter that a cubic regression spline with
one knot at $\xi$ can be obtained using a basis of the form $x, x_2, x_3$,
$(x − \xi)^3_+$, where $(x − \xi)^3_+ = (x − \xi)^3 \quad if \quad x > \xi$ and equals 0 otherwise.
We will now show that a function of the form

$$f (x) = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x − \xi)^3$$

is indeed a cubic regression spline, regardless of the values of $\beta_0, \beta_1, \beta_2, \beta_3, \beta_4$.

(a) Find a cubic polynomial $$f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3$$ such that $f(x) = f_1(x)$ for all $x \leq \xi$. Express $a_1, b_1, c_1, d_1$ in terms of $\beta_0, \beta_1, \beta_2, \beta_3, \beta_4$.
(b) Find a cubic polynomial
$$f_2(x) = a_2 + b_2x + c_2x^2 + d_2x^3$$
such that $f(x) = f_2(x)$ for all $x > \xi$. Express $a_2, b_2, c_2, d_2$ in terms of $\beta_0, \beta_1, \beta_2, \beta_3, \beta_4$. We have now established that f(x) is a piecewise polynomial.
(c) Show that $f_1(\xi) = f_2(\xi)$. That is, $f(x)$ is continuous at $\xi$.
(d) Show that $f^{'}_1(\xi) = f^{′}_2(\xi)$. That is, $f^′(x)$ is continuous at $\xi$.
(e) Show that $f^{′′}_1(\xi) = f^{′′}_2(\xi)$. That is, $f^{′′}(x)$ is continuous at $\xi$.
Therefore, $f(x)$ is indeed a cubic spline.

*Hint: Parts (d) and (e) of this problem require knowledge of single-variable calculus. As a reminder, given a cubic polynomial 
$$f_1(x) = a_1 + b_1x + c_1x^22 + d_1x^3,$$
the first derivative takes the form
$$f^′_1(x) = b_1 + 2c_1x + 3d_1x^2$$
and the second derivative takes the form
$$f^{′′}_1 (x) = 2c_1 + 6d_1x.$$
*

**Solution**

## Exercise 2
Suppose that a curve $\hat{g}$ is computed to smoothly fit a set of n points
using the following formula:

$$\hat{g} = arg \min_g ( \sum_{i=1}^n (y_i − g(x_i))^2 + \lambda \int [g^{(m)}(x)]^2 dx),$$
where $g^{(m)}$ represents the mth derivative of $g$ (and $g^{(0)} = g$). Provide
example sketches of $\hat{g}$ in each of the following scenarios.
(a) $\lambda = \inf, m = 0$.
(b) $\lambda = \inf, m = 1$.
(c) $\lambda = \inf, m = 2$.
(d) $\lambda = \inf, m = 3$.
(e) $\lambda = 0, m = 3$.

**Solution**

## Exercise 3
Suppose we fit a curve with basis functions $b_1(X) = X$, $b_2(X) = (X − 1)^2I(X \geq 1)$. (Note that $I(X \geq 1)$ equals 1 for $X \geq 1$ and 0
otherwise.) We fit the linear regression model 
$$Y = \beta_0 + \beta_1b_1(X) + \beta_2b_2(X) + e,$$
and obtain coefficient estimates $\hat{\beta}_0 = 1$, $\hat{\beta}_1 = 1$, $\hat{\beta}_2 = −2$. Sketch the
estimated curve between $X = −2$ and $X = 2$. Note the intercepts, slopes, and other relevant information.

**Solution**

## Exercise 4
Suppose we fit a curve with basis functions $b_1(X) = I(0 \leq X \leq 2) − (X − 1)I(1 \leq X \leq 2)$, $b_2(X) = (X − 3)I(3 \leq X \leq 4) + I(4 < X \leq 5)$.
We fit the linear regression model 
$$Y = \beta_0 + \beta_1b_1(X) + \beta_2b_2(X) + e,$$
and obtain coefficient estimates $\hat{\beta}_0 = 1$, $\hat{\beta}_1 = 1$, $\hat{\beta}_2 = 3$. Sketch the estimated curve between $X = −2$ and $X = 2$. Note the intercepts, slopes, and other relevant information.

**Solution**

## Exercise 5
Consider two curves, $\hat{g}_1$ and \hat{g}_2, defined by
$$\hat{g}_1 = arg \min_g ( \sum_{i=1}^n (y_i − g(x_i))^2 + \lambda \int [g^{(3)}(x)]^2 dx),$$

$$\hat{g}_2 = arg \min_g ( \sum_{i=1}^n (y_i − g(x_i))^2 + \lambda \int [g^{(4)}(x)]^2 dx),$$

where $g^{(m)}$ represents the mth derivative of $g$.

(a) As $\lambda \to \inf$, will $\hat{g}_1$ or $\hat{g}_2$ have the smaller training RSS?
(b) As $\lambda \to \inf$, will $\hat{g}_1$ or $\hat{g}_2$ have the smaller test RSS?
(c) For $\lambda \to 0$, will $\hat{g}_1$ or $\hat{g}_2$ have the smaller training and test RSS?

**Solution**

## Exercise 6
In this exercise, you will further analyze the Wage data set considered
throughout this chapter.
(a) Perform polynomial regression to predict wage using age. Use
cross-validation to select the optimal degree d for the polyno-
mial. What degree was chosen, and how does this compare to
the results of hypothesis testing using ANOVA? Make a plot of
the resulting polynomial fit to the data.
(b) Fit a step function to predict wage using age, and perform cross-
validation to choose the optimal number of cuts. Make a plot of
the fit obtained.

**Solution**

## Exercise 7
The Wage data set contains a number of other features not explored
in this chapter, such as marital status (maritl), job class (jobclass),
and others. Explore the relationships between some of these other
predictors and wage, and use non-linear fitting techniques in order to
fit flexible models to the data. Create plots of the results obtained,
and write a summary of your findings.

**Solution**

## Exercise 8
Fit some of the non-linear models investigated in this chapter to the
Auto data set. Is there evidence for non-linear relationships in this
data set? Create some informative plots to justify your answer.

**Solution**

## Exercise 9
This question uses the variables dis (the weighted mean of distances
to five Boston employment centers) and nox (nitrogen oxides concen-
tration in parts per 10 million) from the Boston data. We will treat
dis as the predictor and nox as the response.
(a) Use the poly() function to fit a cubic polynomial regression to
predict nox using dis. Report the regression output, and plot
the resulting data and polynomial fits.
(b) Plot the polynomial fits for a range of different polynomial de-
grees (say, from 1 to 10), and plot the associated residual sum
of squares.
(c) Perform cross-validation or another approach to select the opti-
mal degree for the polynomial, and explain your results.
(d) Use the bs() function to fit a regression spline to predict nox
using dis. Report the output for the fit using four degrees of
freedom. How did you choose the knots? Plot the resulting fit.
(e) Now fit a regression spline for a range of degrees of freedom, and
plot the resulting fits as well as the resulting RSS. Describe the
results obtained.
(f) Perform cross-validation or another approach in order to select
the best degrees of freedom for a regression spline on this data.
Describe your results.

**Solution**

## Exercise 10
This question relates to the College data set.
(a) Using out-of-state tuition as the response and the other variables
as the predictors, perform forward stepwise selection in order
to identify a satisfactory model that uses just a subset of the
predictors.
(b) Divide the observations into a training set and a test set. Fit
a GAM on the training data, using out-of-state tuition as the
response and the features selected in the previous step as the
predictors. Plot the results, and explain your findings.
(c) Evaluate the model obtained on the test set, and explain the
results obtained.
(d) For which variables, if any, is there evidence of a non-linear
relationship with the response?

**Solution**

## Exercise 11
In Section 7.7, it was mentioned that GAMs are generally fit using
a backfitting approach. The idea behind backfitting is actually quite
7.9 Exercises 305
simple. We will now explore backfitting in the context of multiple
linear regression.
Suppose that we would like to perform multiple linear regression, but
we do not have software to do so. Instead, we only have software to
perform simple linear regression. Therefore, we take the following it
erative approach: we repeatedly hold all but one coefficient estimate
fixed at its current value, and update only that coefficient estimate
using a simple linear regression. The process is continued until con
vergence — that is, until the coefficient estimates stop changing.
We now try this out on a toy example.

(a) Generate a response Y and two predictors $X_1$ and $X_2$, with$n = 100$.
(b) Initialize ˆβ1 to take on a value of your choice. It does not matter
what value you choose.
(c) Keeping $\hat{\beta}_1$ fixed, fit the model
$Y − \hat{\beta}_1X_1 = \hat{\beta}_0 + \hat{\beta}_2X^2 + e$.
You can do this as follows:
```{r eval=FALSE}
a = y - beta1 * x1
beta2 = lm(a∼x2) $coef[2]
```
(d) Keeping $\hat{\beta}_2$ fixed, fit the model
$Y − \hat{\beta}_2X^2 = \hat{\beta}_0 + \hat{\beta}_1X^1 + e$.
You can do this as follows:
```{r eval=FALSE}
a = y - beta2 * x2 
beta1 = lm(a∼x1) $coef[2]
```

(e) Write a for loop to repeat (c) and (d) 1000 times. Report the
estimates of $\hat{\beta}_0$, $\hat{\beta}_1$, and $\hat{\beta}_2$ at each iteration of the for loop.
Create a plot in which each of these values is displayed, with ˆβ0,
$\hat{\beta}_1$, and $\hat{\beta}_2$ each shown in a different color.
(f) Compare your answer in (e) to the results of simply performing
multiple linear regression to predict $Y$ using $X_1$ and $X_2$. Use
the abline() function to overlay those multiple linear regression
coefficient estimates on the plot obtained in (e).
(g) On this data, set, how many backfitting iterations were required
in order to obtain a “good” approximation to the multiple regression coefficient estimates?

**Solution**

## Exercise 12
This problem is a continuation of the previous exercise. In a toy
example with $p = 100$, show that one can approximate the multiple
linear regression coefficient estimates by repeatedly performing simple
linear regression in a backfitting procedure. How many backfitting
iterations are required in order to obtain a “good” approximation to
the multiple regression coefficient estimates? Create a plot to justify
your answer

**Solution**


# GAM
This question is about using gam for univariate smoothing, the advantages of penalized regression and weighting a smooth model fit. The mcycle data in the MASS package are a classic dataset in univariate smoothing, introduced in Silverman (1985). The data measure the acceleration of the rider’s head, against time, in a simulated motorcycle crash.

1. Plot the acceleration against time, and use gam to fit a univariate smooth to the data, selecting the smoothing parameter by GCV (k of 30 to 40 is plenty for this example). Plot the resulting smooth, with partial residuals, but without standard errors.

2. Use lm and poly to fit a polynomial to the data, with approximately the same degrees of freedom as was estimated by gam. Use termplot to plot the estimated polynomial and partial residuals. Note the substantially worse fit achieved by the polynomial, relative to the penalized regression spline fit.

3. It’s possible to overstate the importance of penalization in explaining the improvement of the penalized regression spline, relative to the polynomial. Use gam to refit an un-penalized thin plate regression spline to the data, with basis dimension the same as that used for the polynomial, and again produce a plot for comparison with the previous two results.

4. Redo part 3 using an un-penalized cubic regression spline. You should find a fairly clear ordering of the acceptability of the results for the four models tried - what is it?

5. Now plot the model residuals against time, and comment.

6. Fit a linear model including a b-spline using the function bs on times and select a suitable degree and the knots position. Compare this model with the previous ones and comment.

**Solution**