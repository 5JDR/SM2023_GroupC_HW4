---
title: "GroupC_HW4"
output: 
  html_document:
    toc: true
editor_options:
  markdown:
    wrap: 72
date: "2023-12-22"
---

# FSDS - Chapter 4
## Exercise 4.24
Refer to the vegetarian survey result in Exercise 4.6, with $n = 25$ and no vegetarians.
(a) Find the Bayesian estimate of π using a beta prior distribution with $\alpha = \beta$ equal 
(i) 0.5,
(ii) 1.0, 
(iii) 10.0. 
Explain how the choice of prior distribution affects the posterior mean estimate.
(b) If you were planning how to take a larger survey from the same population, explain how
you can use the posterior results of the previous survey with $n = 25$ based on the prior
with $\alpha = \beta = 1$ to form the prior distribution to use with the new survey results.

**Solution**

```{r}
n = 25
vegetarian = rep(0, n)
```

```{r}
alpha <- beta <- 0.5
curve(dbeta(x, alpha, beta), ylab="Density", xlim=c(-0.01,0.5), ylim=c(0, 200))
curve(dbeta(x, alpha + sum(vegetarian), beta + n - sum(vegetarian)), add=TRUE, col="red")
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "red"), lty=1)
```

```{r}
alpha <- beta <- 1
curve(dbeta(x, alpha, beta), ylab="Density", xlim=c(-0.01,0.5), ylim=c(0, 30))
curve(dbeta(x, alpha + sum(vegetarian), beta + n - sum(vegetarian)), add=TRUE, col="red")
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "red"), lty=1)
```

```{r}
alpha <- beta <- 10
curve(dbeta(x, alpha, beta), ylab="Density", xlim=c(-0.01,1), ylim=c(0, 10))
curve(dbeta(x, alpha + sum(vegetarian), beta + n - sum(vegetarian)), add=TRUE, col="red")
legend("topright", legend=c("Prior", "Posterior"), col=c("black", "red"), lty=1)
```
The posterior mean will approach the prior mean, so using the value 0.5 and 1 the mean will be near to zero and, using the value 10, the posterior mean will be between zero and the prior mean. We can notice that the posterior mean is 0.2 so the data mean weights more than the prior mean.

####################
######TO CHECK######
####################

```{r}
x <- seq(0, 1, 100)
values <- dbeta(x, 1 + sum(vegetarian), 1 + n - sum(vegetarian))
maximum <- max(values)
p <- x[values == maximum] # Equal to zero

# Always zero
new <- rbinom(10, 1, p)
new
```

## Exercise 4.62
For the bootstrap method, explain the similarity and difference between the true sampling
distribution of $\hat{\theta}$ and the empirically-generated bootstrap distribution in terms of its center
and its spread.

**Solution**

The true sampling distribution is the distribution of the statistic obtained if we infinitely sample from the population. The bootstrap distribution is the distribution of the statistic obtained if we sample from the dataset. For the CLT the centers of both the distributions will be similar, but the spread of the bootstrap distribution will be smaller than the true sampling distribution because based on a limited amount of elements. Therefore, the bootstrap distribution will be affect from the original dataset bias.


# FSDS - Chapter 8
## Exercise 8.4
Refer to Exercise 8.1. Construct a classification tree, and prune strongly until the tree uses
a single explanatory variable. Which crabs were predicted to have satellites? How does the
proportion of correct predictions compare with the more complex tree in Figure 8.2?

**Solution**

```{r message=FALSE, warning=FALSE}
library(rpart) #for fitting decision trees
library(rpart.plot) #for plotting decision trees
```

```{r}
crab <- read.table("crabs.txt", header = TRUE)

fit <- rpart(y ~ weight + width + color + spine, method="class", data=crab)
```

```{r}
pruned_fit <- prune(fit, cp=fit$cptable[4, "CP"])
rpart.plot(pruned_fit, extra=1, digits=4, box.palette="auto")
```


```{r}
pruned_tree <- prune(fit, cp=fit$cptable[2, "CP"])
rpart.plot(pruned_tree, extra=1, digits=4, box.palette="auto")
```

```{r}
y_hat_fit <- apply(predict(pruned_fit, crab), 1, which.max) - 1
y_hat_tree <- apply(predict(pruned_tree, crab), 1, which.max) - 1
```


```{r}
mean(crab$y == y_hat_fit)
mean(crab$y == y_hat_tree)
```

The crabs which were predicted to have satellites are the ones with width >= 25.85. The proportion of correct predictions for the simple tree is 0.7 which is less than the proportion of correct predictions for the more complex tree in figure 8.2 which is 0.75.


# LAB
LAB: Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i∼Exponential(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

$\pi(\lambda)=Beta(4,2);$

$\pi(\lambda)=Normal(1,2);$

$\pi(\lambda)=Gamma(4,2);$

Now, compute your posterior as $\pi(\lambda|y) \propto L(\lambda; y)\pi(\lambda)$ for the selected prior. If your first choice was correct, you will be able to compute it analytically. \newline

The parameter of the exponential distribution is strictly positive and defined on the whole real line. This implies that a suitable prior distribution for $\lambda$ is given by the Gamma random variable.

$f_{y;\lambda} = \lambda^n e^{-\lambda \sum y_i}$

$f_{\lambda} = \frac{1}{\Gamma(\alpha)}\beta^{\alpha}\lambda^{\alpha-1}e^{-\beta \lambda}$

$f_{\lambda|y} \propto \lambda^n e^{-\lambda \sum y_i} \lambda^{\alpha - 1} e^{-\beta \lambda} = \lambda^{n + \alpha+1}e^{-\lambda(\sum y_i + \beta)}$

Which is the kernel of a Gamma distribution with parameters $\alpha + n$ and $\sum y_i + \beta$.


**Solution**


# ISLR - Chapter 7 - Exercises 7.9
## Exercise 1
It was mentioned in the chapter that a cubic regression spline with
one knot at $\xi$ can be obtained using a basis of the form $x, x^2, x^3$,
$(x − \xi)^3_+$, where $(x − \xi)^3_+ = (x − \xi)^3 \quad if \quad x > \xi$ and equals 0 otherwise.
We will now show that a function of the form

$$f (x) = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x − \xi)^3$$

is indeed a cubic regression spline, regardless of the values of $\beta_0, \beta_1, \beta_2, \beta_3, \beta_4$.

(a) Find a cubic polynomial $$f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3$$ such that $f(x) = f_1(x)$ for all $x \leq \xi$. Express $a_1, b_1, c_1, d_1$ in terms of $\beta_0, \beta_1, \beta_2, \beta_3, \beta_4$.
(b) Find a cubic polynomial
$$f_2(x) = a_2 + b_2x + c_2x^2 + d_2x^3$$
such that $f(x) = f_2(x)$ for all $x > \xi$. Express $a_2, b_2, c_2, d_2$ in terms of $\beta_0, \beta_1, \beta_2, \beta_3, \beta_4$. We have now established that f(x) is a piecewise polynomial.
(c) Show that $f_1(\xi) = f_2(\xi)$. That is, $f(x)$ is continuous at $\xi$.
(d) Show that $f^{'}_1(\xi) = f^{′}_2(\xi)$. That is, $f^′(x)$ is continuous at $\xi$.
(e) Show that $f^{′′}_1(\xi) = f^{′′}_2(\xi)$. That is, $f^{′′}(x)$ is continuous at $\xi$.
Therefore, $f(x)$ is indeed a cubic spline.

*Hint: Parts (d) and (e) of this problem require knowledge of single-variable calculus. As a reminder, given a cubic polynomial 
$$f_1(x) = a_1 + b_1x + c_1x^22 + d_1x^3,$$
the first derivative takes the form
$$f^′_1(x) = b_1 + 2c_1x + 3d_1x^2$$
and the second derivative takes the form
$$f^{′′}_1 (x) = 2c_1 + 6d_1x.$$
*

**Solution**

(a) Given that for $x \leq \xi$ we have that $(x − \xi)^3_+ = 0$, then we can write $f(x)$ as:

$$
f(x) = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4 \cdot 0 = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3
$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Then, the coefficients of $f_1(x)$ are:

- $a_1 = \beta_0$
- $b_1 = \beta_1$
- $c_1 = \beta_2$
- $d_1 = \beta_3$

(b) Given that for $x > \xi$ we have that $(x − \xi)^3_+ = (x − \xi)^3$, then we can write $f(x)$ as:

$$
f(x) = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x − \xi)^3
$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If we expand the last term we get:

$$
\beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x^3 - 3 x^2 \xi + 3 \xi^2 x - \xi^3) = \\ = (\beta_0 - \beta_4\xi^3) + (\beta_1 + 3\beta_4\xi^2)x + (\beta_2 - 3\beta_4\xi)x^2 + (\beta_3 + \beta_4)x^3 
$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Then, the coefficients of $f_2(x)$ are:

- $a_2 = \beta_0 - \beta_4\xi^3$
- $b_2 = \beta_1 + 3\beta_4\xi^2$
- $c_2 = \beta_2 - 3\beta_4\xi$
- $d_2 = \beta_3 + \beta_4$

(c)

$$
f_1(\xi) = a_1 + b_1 \xi + c_1 \xi^2 + d_1 \xi^3 = \\ = \beta_0 + \beta_1\xi + \beta_2\xi^2 + \beta_3\xi^3
$$

$$
f_2(\xi) = a_2 + b_2 \xi + c_2 \xi^2 + d_2 \xi^3 = \\ = (\beta_0 - \beta_4\xi^3) + (\beta_1 + 3\beta_4\xi^2)\xi + (\beta_2 - 3\beta_4\xi)\xi^2 + (\beta_3 + \beta_4)\xi^3 = \\ = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3 - \beta_4\xi^3 + \beta_4\xi^3 + 3\beta_4\xi^3 - 3\beta_4\xi^3 = \\ = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3
$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f_1(\xi) = f_2(\xi)$, $q. e. d.$

(d)

$$
f^{'}_1(\xi) = b_1 + 2c_1\xi + 3d_1\xi^2 = \\ = \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2
$$

$$
f^{'}_2(\xi) = b_2 + 2c_2\xi + 3d_2\xi^2 = \\ = (\beta_1 + 3\beta_4\xi^2) + 2(\beta_2 - 3\beta_4\xi)\xi + 3(\beta_3 + \beta_4)\xi^2 = \\ = \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2 + 3\beta_4\xi^2 - 6\beta_4\xi^2 + 3\beta_4\xi^2 = \\ = \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2
$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f^{'}_1(\xi) = f^{'}_2(\xi)$, $q. e. d.$

(e)

$$
f^{''}_1(\xi) = 2c_1 + 6d_1\xi = \\ = 2\beta_2 + 6\beta_3\xi
$$
$$
f^{''}_2(\xi) = 2c_2 + 6d_2\xi = \\ = 2(\beta_2 - 3\beta_4\xi) + 6(\beta_3 + \beta_4)\xi = \\ = 2\beta_2 + 6\beta_3\xi - 6\beta_4\xi + 6\beta_4\xi = \\ = 2\beta_2 + 6\beta_3\xi
$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f^{''}_1(\xi) = f^{''}_2(\xi)$, $q. e. d.$

Therefore:

- Since $f(x)$ can be written as a linear combination of $f_1(x)$ and $f_2(x)$
- which are two cubic polynomials, defined for $x \leq \xi$ and for $x > \xi$, 
- $f_1(x)$ and $f_2(x)$ are continuous and have continuous first and second derivatives,

then $f(x)$ is a cubic spline.

## Exercise 2
Suppose that a curve $\hat{g}$ is computed to smoothly fit a set of n points
using the following formula:

$$\hat{g} = arg \min_g ( \sum_{i=1}^n (y_i − g(x_i))^2 + \lambda \int [g^{(m)}(x)]^2 dx),$$
where $g^{(m)}$ represents the mth derivative of $g$ (and $g^{(0)} = g$). Provide
example sketches of $\hat{g}$ in each of the following scenarios.
(a) $\lambda = \inf, m = 0$.
(b) $\lambda = \inf, m = 1$.
(c) $\lambda = \inf, m = 2$.
(d) $\lambda = \inf, m = 3$.
(e) $\lambda = 0, m = 3$.

**Solution**

(a) If $\lambda = \inf$ and $m = 0$, then we have that $\hat{g} = arg \min_g (\sum_{i=1}^n (y_i − g(x_i))^2)$, which reduce the value of the function $g$, then the result will be a constant function equal to zero.
(b) If $\lambda = \inf$ and $m = 1$, then we have that $\hat{g} = arg \min_g (\sum_{i=1}^n (y_i − g(x_i))^2 + \lambda \int [g^{(1)}(x)]^2 dx)$, which is the least squares fit with a penalty on the first derivative. The penalty term is so high that the fit is a straight line.
(c) If $\lambda = \inf$ and $m = 2$, then we have that $\hat{g} = arg \min_g (\sum_{i=1}^n (y_i − g(x_i))^2 + \lambda \int [g^{(2)}(x)]^2 dx)$, which is the least squares fit with a penalty on the second derivative. The penalty term is so high that the fit is a parabolic line.
(d) If $\lambda = \inf$ and $m = 3$, then we have that $\hat{g} = arg \min_g (\sum_{i=1}^n (y_i − g(x_i))^2 + \lambda \int [g^{(3)}(x)]^2 dx)$, which is the least squares fit with a penalty on the third derivative. The penalty term is so high that the fit is a cubic line.
(e) If $\lambda = 0$ and $m = 3$, then we have that $\hat{g} = arg \min_g ( \sum_{i=1}^n (y_i − g(x_i))^2 + \lambda \int [g^{(3)}(x)]^2 dx)$, which is the least squares fit with no penalty because the factor $\lambda$ is zero.

## Exercise 3
Suppose we fit a curve with basis functions $b_1(X) = X$, $b_2(X) = (X − 1)^2I(X \geq 1)$. (Note that $I(X \geq 1)$ equals 1 for $X \geq 1$ and 0
otherwise.) We fit the linear regression model 
$$Y = \beta_0 + \beta_1b_1(X) + \beta_2b_2(X) + e,$$
and obtain coefficient estimates $\hat{\beta}_0 = 1$, $\hat{\beta}_1 = 1$, $\hat{\beta}_2 = −2$. Sketch the
estimated curve between $X = −2$ and $X = 2$. Note the intercepts, slopes, and other relevant information.

**Solution**

```{r}
Identity <- function(x) {
  if (x >= 1) {
    return(1)
  } else {
    return(0)
  }
}
```


```{r}
x <- seq(-2, 2, 0.1)

y <- 1 + x - 2 * (x - 1)^2 * sapply(x, Identity)

plot(x, y, type = "l")
```


We can see that if $x \leq 1 $the intercept is $\beta_0$ and the slope is $\beta_1$. In this case the function is a line.
If $x > 1$ the intercept is $\beta_0 + \beta_2$ and the slope is $\beta_1 - 4\beta_2$ and the coefficient of the quadratic term is equal to $\beta_2$. In this case the function is a parabola.


## Exercise 4
Suppose we fit a curve with basis functions $b_1(X) = I(0 \leq X \leq 2) − (X − 1)I(1 \leq X \leq 2)$, $b_2(X) = (X − 3)I(3 \leq X \leq 4) + I(4 < X \leq 5)$.
We fit the linear regression model 
$$Y = \beta_0 + \beta_1b_1(X) + \beta_2b_2(X) + e,$$
and obtain coefficient estimates $\hat{\beta}_0 = 1$, $\hat{\beta}_1 = 1$, $\hat{\beta}_2 = 3$. Sketch the estimated curve between $X = −2$ and $X = 2$. Note the intercepts, slopes, and other relevant information.

**Solution**

```{r}
Identity_range <- function(x, l, u) {
  if (x <= u && x >= l) {
    return(1)
  } else {
    return(0)
  }
}

b1 <- function(x) {
  return(Identity_range(x, 0, 2) - (x - 1) * Identity_range(x, 1, 2))
}

b2 <- function(x) {
  return((x - 3) * Identity_range(x, 3, 4) + Identity_range(x, 4, 5))
}
```

```{r}
x <- seq(-2, 2, 0.1)

y <- 1 + x * sapply(x, b1) + 3 * sapply(x, b2)

plot(x, y, type = "l")
```

If $x < 0$ or $2 < x < 3$ or $x > 5$ the function is the constant value $\beta_0$.
If $0 \leq x < 1$ the function is a line with intercept $\beta_0$ and slope $\beta_1$.
If $1 \leq x \leq 2$ the function is a constant value equal to $\beta_0 + \beta_1$.
If $3 \leq x \leq 4$ the function is a line with intercept $\beta_0 + 3\beta_2$ and slope $\beta_2$.
If $4 < x \leq 5$ the function is a constant value equal to $\beta_0 + \beta_2$.

## Exercise 5
Consider two curves, $\hat{g}_1$ and $\hat{g}_2$, defined by
$$\hat{g}_1 = arg \min_g ( \sum_{i=1}^n (y_i − g(x_i))^2 + \lambda \int [g^{(3)}(x)]^2 dx),$$

$$\hat{g}_2 = arg \min_g ( \sum_{i=1}^n (y_i − g(x_i))^2 + \lambda \int [g^{(4)}(x)]^2 dx),$$

where $g^{(m)}$ represents the mth derivative of $g$.

(a) As $\lambda \to \inf$, will $\hat{g}_1$ or $\hat{g}_2$ have the smaller training RSS?
(b) As $\lambda \to \inf$, will $\hat{g}_1$ or $\hat{g}_2$ have the smaller test RSS?
(c) For $\lambda \to 0$, will $\hat{g}_1$ or $\hat{g}_2$ have the smaller training and test RSS?

**Solution**

(a) In the limit of $\lambda \to \inf$ the penalty term will dominate the RSS. For this reason the resulting function will be the most smooth possible. This means that both the functions will have the same training RSS. But the function $\hat{g}_2$ will have the greater training RSS because uses a greater derivative order.
(b) As before, in the limit of $\lambda \to \inf$ the penalty term will dominate the RSS. For this reason the resulting function will be the most smooth possible. This means that both the functions will have the same test RSS. But the function $\hat{g}_2$ will have the greater test RSS because uses a greater derivative order.
(c) In the limit of $\lambda \to 0$ the penalty term will be negligible. For this reason the resulting function will be the most flexible possible. This means that both the functions will have the same training and test RSS.

## Exercise 6
In this exercise, you will further analyze the Wage data set considered
throughout this chapter.

(a) Perform polynomial regression to predict wage using age. Use
cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to
the results of hypothesis testing using ANOVA? Make a plot of
the resulting polynomial fit to the data.
(b) Fit a step function to predict wage using age, and perform cross-validation to choose the optimal number of cuts. Make a plot of
the fit obtained.

**Solution**

```{r}
library(ISLR)
library(modelr)
cv  <- crossv_kfold(Wage, k = 10)

data <- Wage
models <- vector("list", 10)
RMSE <- rep(NA, 10)
R2 <- rep(NA, 10)
```


```{r}
results.df <- data.frame("degree"=rep(NA, 5), "RMSE"=rep(NA, 5), "R2"=rep(NA, 5))

for (d in 1:5){
  for (i in 1:10) {
    model <- lm(wage ~ poly(age, d), data = cv$train[[i]])
    
    y_hat <- predict(model, cv$test[[i]])
    RMSE[i] <- sqrt(mean((y_hat - data$wage[cv$test[[i]][2]$idx])^2))
    
    R2[i] <- summary(model)$adj.r.squared
    
  }
  
  models[[d]] <- model
  results.df[d, ] <- list("degree"=d, "RMSE"=mean(RMSE), "R2"=mean(R2))
}

results.df
```

```{r}
anova(models[[1]], models[[2]], models[[3]], models[[4]], models[[5]])
```

From the results we can see that the best degree is 4 if we are considering the RMSE and the R2 values. If we are considering the ANOVA test then the best degree is 2, which RMSE and R2 are really similar. The plot of the resulting polynomial fit to the data is the following:

```{r}
x <- seq(18, 80, length.out=100)
poly2 <- lm(wage ~ poly(age, 2), data = Wage)
y_hat <- predict(poly2, data.frame(age = x))

plot(data$age, data$wage, col = "blue", pch = 20, cex = 0.5)
lines(x, y_hat, col = "red", lwd = 2)
```
```{r}
results.df <- data.frame("degree"=rep(NA, 10), "RMSE"=rep(NA, 10), "R2"=rep(NA, 10))

for (d in 3:10){
  for (i in 1:10) {
    model <- lm(wage ~ cut(age, seq(1, 80, length.out=d)), data = cv$train[[i]])
    
    y_hat <- predict(model, cv$test[[i]])
    RMSE[i] <- sqrt(mean((y_hat - data$wage[cv$test[[i]][2]$idx])^2))
    
    R2[i] <- summary(model)$adj.r.squared
    
  }
  
  models[[d]] <- model
  results.df[d, ] <- list("degree"=d, "RMSE"=mean(RMSE), "R2"=mean(R2))
}

results.df[c(-1, -2), ]
```
The best result is obtained with 8 cuts. This results are really similar to the previous polynomial models. The plot of the fit obtained is the following:
```{r}
x <- seq(18, 80, length.out=100)
cut8 <- lm(wage ~ cut(age, seq(1, 80, length.out=8)), data = Wage)
y_hat <- predict(cut8, data.frame(age = x))

plot(data$age, data$wage, col = "blue", pch = 20, cex = 0.5)
lines(x, y_hat, col = "red", lwd = 2)
```


## Exercise 7
The Wage data set contains a number of other features not explored
in this chapter, such as marital status (maritl), job class (jobclass),
and others. Explore the relationships between some of these other
predictors and wage, and use non-linear fitting techniques in order to
fit flexible models to the data. Create plots of the results obtained,
and write a summary of your findings.

**Solution**

```{r}
library(ISLR)

summary(Wage)
```

The dataset contains 12 variables, 7 of them are categorical. The variable region is not significant because all the rows have the same value. We can try to fit a linear model and then try to fit a non-linear models using all the variable beside region and logwage.
```{r}
data <- Wage[, -c(6, 10)]

complete <- lm(wage ~ ., data = data)
summary(complete)
```

```{r}
poly <- lm(wage ~ poly(age, 2) + poly(year, 3) + maritl + race + education + jobclass + health_ins, data = data)

summary(poly)
```
We can notice that the polynomial fit is statistically significant for the variable year, this probably because we have just 6 years. For this reason we can try to remove it.
```{r}
poly2 <- lm(wage ~ poly(age, 2) + year + maritl + race + education + jobclass + health_ins, data = data)

summary(poly2)
```
```{r}
anova(poly2)
```
We can notice that the remaining variables are statistically significant but the R-squared value is increased only by 0.01 (roughly). We can try to use the step function instead of the polynomial fit.
```{r}
cutted <- lm(wage ~ cut(age, seq(1, 80, length.out=8)) + year + maritl + race + education + jobclass + health_ins, data = data)

summary(cutted)
```

In this case the results are slightly better than the previous ones. As last try we can use the splines on the variable age.
```{r message=FALSE, warning=FALSE}
library(mgcv)
smoothed <- gam(wage ~ s(age) + year + maritl + race + education + jobclass + health_ins, data = data)

summary(smoothed)
```

The results are similar to the previous ones. Then the best model is the one obtained with the step function even if the difference is really small. The plot of the fit obtained is the following:
```{r}
sorted <- data[order(data$age), ]
y_hat_complete <- predict(complete, data.frame(sorted))
y_hat_poly     <- predict(poly    , data.frame(sorted))
y_hat_poly2    <- predict(poly2   , data.frame(sorted))
y_hat_cutted   <- predict(cutted  , data.frame(sorted))
y_hat_smoothed <- predict(smoothed, data.frame(sorted))

plot (sorted$age, data$wage     , col = "blue"  , pch = 20, cex = 0.5)
lines(sorted$age, y_hat_complete, col = "red"   , lwd = 2)
lines(sorted$age, y_hat_poly    , col = "green" , lwd = 2)
lines(sorted$age, y_hat_poly2   , col = "orange", lwd = 2)
lines(sorted$age, y_hat_cutted  , col = "purple", lwd = 2)
lines(sorted$age, y_hat_smoothed, col = "black" , lwd = 2)
legend("topleft", legend = c("Complete", "Poly", "Poly2", "Cutted", "Smoothed"), col = c("red", "green", "orange", "purple", "black"), lty = 1, cex = 0.8)
```

From the plot we can notice that the models are really similar but all of them are not smoothed at all.


## Exercise 8
Fit some of the non-linear models investigated in this chapter to the
Auto data set. Is there evidence for non-linear relationships in this
data set? Create some informative plots to justify your answer.

**Solution**

Let's first visualize the data to look for non-linear relationships.

```{r}
library(ISLR)
library(tidyr)
library(dplyr)

plot(Auto)
```

It looks like many of the variables have non-linear relationships with mpg (the response). Let's try fitting some non-linear models to the data.

### Polynomial Regression

Using `horsepower` as the only predictor.

By doing 8-fold cross-validation, we want to find out which degree of polynomial fits the data best without overfitting.

```{r, warning=FALSE}
# Selecting the best fit by K-fold cross validation
library(ggplot2)

folds <- sample(rep(1:8, nrow(Auto)/8))
errors <- matrix(NA, 8, 8)
models <- list()

for (k in 1:8) {
    for (i in 1:8) {
        model <- lm(mpg ~ poly(horsepower,i), data = Auto[folds != i,])
        pred <- predict(model, Auto[folds == i,])
        resid <- (Auto$mpg[folds==i] - pred)^2
        errors[k, i] <- sqrt(mean(resid))
    }
}
errors <- apply(errors, 2, mean)

data_frame(RMSE = errors) %>%
    mutate(Polynomial = row_number()) %>%
    ggplot(aes(Polynomial, RMSE, fill = Polynomial == which.min(errors))) + 
    geom_col() +
    scale_x_continuous(breaks = 1:8) +
    guides(fill = FALSE) +
    coord_cartesian(ylim = c(min(errors), max(errors)))

optimal_degree <- which.min(errors)
```

The optimal polynomial degree is "r optimal_degree".

Let's fit a polynomial regression model with degree "r optimal_degree".

```{r}
poly <- lm(mpg ~ poly(horsepower, optimal_degree), data = Auto)
summary(poly)
```
```{r, warning=FALSE}
Auto %>%
    mutate(Predictions = predict(poly, Auto)) %>%
    ggplot() + xlab('Horsepower') + ylab('MPG') + 
    geom_point(aes(horsepower, mpg, col = 'blue')) +
    geom_line(aes(horsepower, Predictions, col = 'red'), size = 1.5) +
    scale_color_manual(name = 'Value Type',
                       labels = c('Observed', 'Predicted'),
                       values = c('#60B7F8', '#E89F15'))
```

It looks like a polynomial regression model with degree 7 fits the non-linear relationship between `horsepower` and `mpg` well.

### Splines

Using `weight` as the only predictor.

Now mpg will be predicted with a spline function of weight. Also a linear model will be fitted. Then an anova test will be performed to see if the spline model fits the data significantly better than the linear model. If so, it may be that the relationship between weight and mpg is non-linear.

```{r, warning=FALSE, message=FALSE}
# Using Caret package to ease model selection
library(caret)

spline <- train(mpg ~ displacement, data = Auto,
                method = 'gamSpline',
                trControl = trainControl(method = 'cv', number = 8),
                tuneGrid = expand.grid(df = seq(1, 12, 1)))
linear <- train(mpg ~ displacement, data = Auto, method = 'lm',
                trControl = trainControl(method = 'cv', number = 8))
plot(spline)
```
The cross-validation leads us to choose a spline basis with 12 degrees of freedom.

```{r}
anova(linear$finalModel, spline$finalModel)
```

The Spline model fits better the data. It decreases the residual sum of squares by 20% and its p-value is close to zero. Therefore, we can say with good confidence that the relationship is non-linear.

```{r}
Auto %>%
    mutate(pred = predict(spline, Auto)) %>%
    ggplot() + 
    geom_point(aes(displacement, mpg, col = 'blue')) +
    geom_line(aes(displacement, pred, col = 'red'), size = 1.5) +
    scale_color_manual(name = 'Value Type',
                       labels = c('Observed', 'Predicted'),
                       values = c('#60B7F8', '#E89F15')) +
    labs(x = 'Acceleration', y = 'MPG')
```

### Generalized Adaptive Model

Now we will fit a GAM to mpg as a function of horsepower, weight, displacement and acceleration. The same comparison as before with a linear model will be performed.

```{r, warning=FALSE, message=FALSE}
# Crossvalidating again via Caret

gam <- train(mpg ~ horsepower  + 
               weight + 
               displacement +
               acceleration +
               year, data = Auto,
               method = 'gam',
               trControl = trainControl(method = 'cv', number = 10))

linear_2 <- train(mpg ~ horsepower  + 
                    weight + 
                    displacement +
                    acceleration +
                    year +
                    cylinders, data = Auto,
                    method = 'lm',
                    trControl = trainControl(method = 'cv', number = 10))

summary(gam$finalModel)
```

```{r}
anova(linear_2$finalModel, gam$finalModel)
```

The GAM model fits the data better than the linear model. It decreases the residual sum of squares by 42% and its p-value is close to zero. Therefore, we can say with good confidence that the relationship between the response and the covariates is non-linear.

```{r}
Auto %>%
    mutate(pred = predict(gam, Auto)) %>%
    gather(variable, value, 
           weight, horsepower, 
           displacement, acceleration,
           year) %>%
    ggplot() + 
    labs(x = '', y = 'MPG') + 
    geom_point(aes(value, mpg, col = 'blue')) +
    geom_point(aes(value, pred, col = 'red'), size = 1.5) +
    scale_color_manual(name = 'Value Type',
                       labels = c('Observed', 'Predicted'),
                       values = c('#60B7F8', '#E89F15')) +
    facet_wrap(~variable, scales = 'free')
```

## Exercise 9
This question uses the variables dis (the weighted mean of distances
to five Boston employment centers) and nox (nitrogen oxides concen-
tration in parts per 10 million) from the Boston data. We will treat
dis as the predictor and nox as the response.

(a) Use the poly() function to fit a cubic polynomial regression to
predict nox using dis. Report the regression output, and plot
the resulting data and polynomial fits.

```{r}
library(MASS)
library(ggplot2)
library(splines)

data(Boston)

fit = lm(nox ~ poly(dis, 3), data = Boston)

summary(fit)

ggplot(Boston, aes(dis, nox)) + geom_point() + 
  stat_smooth(method = "lm", formula = y ~ poly(x, 3), se = FALSE)
```
(b) Plot the polynomial fits for a range of different polynomial de-
grees (say, from 1 to 10), and plot the associated residual sum
of squares.

```{r}
df = data.frame("nox"=numeric(),
                "dis"=numeric(),
                "degree"=factor())
rss = vector(mode="numeric", length=10L)

for (i in 1:10) {
  fit = lm(nox ~ poly(dis, i), data = Boston)
  
  df1 = data.frame("nox"=fit$fitted.values,
                   "dis"=Boston$dis,
                   "degree"=paste0(i))
  
  df = rbind(df, df1)
  rss[i] = sum(resid(fit)^2)
}

ggplot(data=Boston, aes(x=dis, y=nox)) + geom_point() + 
  geom_line(data=df, aes(x=dis, y=nox, color=degree))

df1 = data.frame("rss"=rss,
                 "p"=1:10)
df1 %>% ggplot(aes(x=p, y=rss)) + geom_point() + geom_line()
```

(c) Perform cross-validation or another approach to select the opti-
mal degree for the polynomial, and explain your results.

```{r}
suppressPackageStartupMessages(library(cv))

mse = vector(mode="numeric", length=10L)
for (p in 1:10){
  fit = lm(nox ~ poly(dis, p), data=Boston)
  mse[p] = cv(fit)$`CV crit`
  
}
df = data.frame("mse"=mse,
                "degree"=1:10)

df %>% ggplot(aes(x=degree, y=mse)) + geom_line() + geom_point()

```
The MSE decreases at first and reaches the minimum for a polynomial of degree $p=3$. With polynomials of degree 6 and above the model fails to predict the values at the left boundary of X (see the previous plots), thus increasing noticeably the MSE. \newline

(d) Use the bs() function to fit a regression spline to predict nox
using dis. Report the output for the fit using four degrees of
freedom. How did you choose the knots? Plot the resulting fit.

```{r}
fit = lm(nox ~ bs(dis, df=4), data = Boston)

summary(fit)

# The knots are picked by default by the bs() function at quantiles 0.2, 0.4, 0.6, 0.8 of X.
ggplot(Boston, aes(dis, nox)) + geom_point() +
  stat_smooth(method = "lm", formula = y ~ bs(x, df=4), se = FALSE)
```
(e) Now fit a regression spline for a range of degrees of freedom, and
plot the resulting fits as well as the resulting RSS. Describe the
results obtained.

```{r}
df = data.frame("nox"=numeric(),
                "dis"=numeric(),
                "df"=factor())
rss = c()

for (i in seq(3,15,2)) {
  fit = lm(nox ~ bs(dis, df=i), data = Boston)
  
  df1 = data.frame("nox"=fit$fitted.values,
                   "dis"=Boston$dis,
                   "df"=paste0(i))
  
  df = rbind(df, df1)
  rss = c(rss, sum(resid(fit)^2))
}

ggplot(data=Boston, aes(x=dis, y=nox)) + geom_point() + 
  geom_line(data=df, aes(x=dis, y=nox, color=df))

df1 = data.frame("rss"=rss,
                 "df"=seq(3,15,2))
df1 %>% ggplot(aes(x=df, y=rss)) + geom_point() + geom_line()
```
An excessive number of degrees of freedom results in an overfit of the data (again, on the left bound of X), and unnaturally wiggly regression lines. \newline

(f) Perform cross-validation or another approach in order to select
the best degrees of freedom for a regression spline on this data.
Describe your results.

```{r}
mse = c()
for (p in seq(3,14,2)){
  fit = lm(nox ~ poly(dis, p), data=Boston)
  mse = c(mse, cv(fit)$`CV crit`)
  
}
df = data.frame("mse"=mse,
                "df"=seq(3,14,2))

df %>% ggplot(aes(x=df, y=mse)) + geom_line() + geom_point()
```

Once again, an excessive number of degrees of freedom leads to overfitting the train data leading to poor performance on the test set, and therefore poor generalization power. 


## Exercise 10
This question relates to the College data set.
(a) Using out-of-state tuition as the response and the other variables
as the predictors, perform forward stepwise selection in order
to identify a satisfactory model that uses just a subset of the
predictors.
(b) Divide the observations into a training set and a test set. Fit
a GAM on the training data, using out-of-state tuition as the
response and the features selected in the previous step as the
predictors. Plot the results, and explain your findings.
(c) Evaluate the model obtained on the test set, and explain the
results obtained.
(d) For which variables, if any, is there evidence of a non-linear
relationship with the response?

**Solution**

(a)

```{r, warning=FALSE, message=FALSE}
library(ISLR)
library("leaps")
library("glmnet")

set.seed(0)

# Divide the dataset into training (1) and test (2)
dataset_part <- sample(1:2, nrow(College), replace = T, prob = c(0.8, 0.2))

p <- ncol(College) - 1

# Fit subsets of various sizes:
regfit.forward <- regsubsets(Outstate ~ ., data = College[dataset_part == 1, ], nvmax = p, method = "forward")

# Test the trained models on the test set:
test.mat <- model.matrix(Outstate ~ ., data = College[dataset_part == 2, ])
test.errors <- rep(NA, p)
for (i in 1:p) {
    coefi <- coef(regfit.forward, id = i)
    pred <- test.mat[, names(coefi)] %*% coefi
    test.errors[i] <- mean((College$Outstate[dataset_part == 2] - pred)^2)
}

k <- which.min(test.errors)

plot(test.errors, xlab = "Number of variables", ylab = "Validation MSE", pch = 19, type = "b")
abline(v = k, col = "red")

k
```

It looks like the best model is the one with 9 predictors, given by the validation errors.

The variables (and their coefficients) are:

```{r, warning=FALSE, message=FALSE}
coef(regfit.forward, id = k)
```

(b)

```{r, warning=FALSE, message=FALSE}
library(gam)

gam_fit <- gam(Outstate ~ Private + Accept + Room.Board + Personal + Terminal + S.F.Ratio + perc.alumni + Expend + Grad.Rate, data = College[dataset_part == 1, ])
```

****What should we plot? Should we say something else?****

(c)

```{r}
# Predict the GAM performance on the test dataset:
y_hat <- predict(gam_fit, newdata = College[dataset_part == 2, ])
RMSE <- mean((College[dataset_part == 2, ]$Outstate - y_hat)^2)

cat("RMSE of the GAM model:", RMSE, ", RMSE of the best subset model:", test.errors[k], "\n")
```

Surprisingly, the GAM model and the best subset model have the same RMSE.

****WHY?****

(d)

```{r, warning=FALSE, message=FALSE, fig.height=10, fig.width=10}
par(mfrow = c(3, 3))

par(mfrow = c (4, 4))
for(var in names(College)){
  if(var != "Outstate" && var != "Private") # Avoid plotting the response variable and the categorical variables
    plot(College[, var], College$Outstate, xlab = var, ylab = "Outstate", pch = 19, col = "blue4")
}
```

It's not very clear which variables are in a non-linear relationship with the response due to the high variability of the data. However, some of them show some non-linear trends.

## Exercise 11
In Section 7.7, it was mentioned that GAMs are generally fit using a backfitting approach. The idea behind backfitting is actually quite simple. We will now explore backfitting in the context of multiple linear regression.
Suppose that we would like to perform multiple linear regression, but we do not have software to do so. Instead, we only have software to perform simple linear regression. Therefore, we take the following it erative approach: we repeatedly hold all but one coefficient estimate fixed at its current value, and update only that coefficient estimate using a simple linear regression. The process is continued until convergence — that is, until the coefficient estimates stop changing.
We now try this out on a toy example.

(a) Generate a response Y and two predictors $X_1$ and $X_2$, with$n = 100$.
(b) Initialize ˆβ1 to take on a value of your choice. It does not matter
what value you choose.
(c) Keeping $\hat{\beta}_1$ fixed, fit the model
$Y − \hat{\beta}_1X_1 = \hat{\beta}_0 + \hat{\beta}_2X^2 + e$.
You can do this as follows:
```{r eval=FALSE}
a = y - beta1 * x1
beta2 = lm(a∼x2) $coef[2]
```
(d) Keeping $\hat{\beta}_2$ fixed, fit the model
$Y − \hat{\beta}_2X^2 = \hat{\beta}_0 + \hat{\beta}_1X^1 + e$.
You can do this as follows:
```{r eval=FALSE}
a = y - beta2 * x2 
beta1 = lm(a∼x1) $coef[2]
```

(e) Write a for loop to repeat (c) and (d) 1000 times. Report the
estimates of $\hat{\beta}_0$, $\hat{\beta}_1$, and $\hat{\beta}_2$ at each iteration of the for loop.
Create a plot in which each of these values is displayed, with ˆβ0,
$\hat{\beta}_1$, and $\hat{\beta}_2$ each shown in a different color.
(f) Compare your answer in (e) to the results of simply performing
multiple linear regression to predict $Y$ using $X_1$ and $X_2$. Use
the abline() function to overlay those multiple linear regression
coefficient estimates on the plot obtained in (e).
(g) On this dataset, how many backfitting iterations were required in order to obtain a “good” approximation to the multiple regression coefficient estimates?

**Solution**

(a)

The following model is considered for $Y$:
$$Y=18 + 0.8 X_1 − 20 X_2 + \epsilon \\ X_1 \sim N(0,1) \\ X_2 \sim N(0,1) \\ \epsilon \sim N(0,1)$$

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(gridExtra)

set.seed(591)
x1 <- rnorm(100)
x2 <- rnorm(100)
eps <- rnorm(100)

y <- 18 + 0.8*x1 - 20*x2 + eps
```

```{r}
g1 <- data.frame(x1, x2, y) %>%
  ggplot(aes(x1, y)) + 
  geom_point()

g2 <- data.frame(x1, x2, y) %>%
  ggplot(aes(x2, y)) + 
  geom_point()

grid.arrange(g1, g2, ncol = 2)
```

(b)

\hat{\beta}_1 is initialized to 1.

```{r}
beta1 <- 1
```

(c)

```{r}
a = y - beta1 * x1
beta2 = lm(a~x2)$coef[2]
beta2
```

(d)

```{r}
r1 = y - beta2 * x2
beta1 = lm(r1 ~ x1)$coef[2]
beta1
```

(e)

```{r}
library(tidyverse)

beta_1_init <- 1

beta_0 <- c()
beta_1 <- c()
beta_2 <- c()

# beta_2:
r2 <- y - beta_1_init * x1
beta_2[1] <- lm(r2 ~ x2)$coef[2]

# beta_1:
r1 <- y - beta_2[1] * x2
lm_coef <- lm(r1 ~ x1)$coef
beta_1[1] <- lm_coef[2]

# beta_0:
beta_0[1] <- lm_coef[1]


for (i in 2:1000) {
  r2 <- y - beta_1[i-1] * x1
  beta_2[i] <- lm(r2 ~ x2)$coef[2]
  
  r1 <- y - beta_2[i-1] * x2
  lm_coef <- lm(r1 ~ x1)$coef
  beta_1[i] <- lm_coef[2]
  
  beta_0[i] <- lm_coef[1]
}

data.frame(iteration = 1:1000, beta_0, beta_1, beta_2) %>%
  pivot_longer(-iteration, names_to = "coefficient", values_to = "estimate") %>%
  ggplot(aes(x = iteration, y = estimate, col = factor(coefficient))) + 
  geom_line() + 
  scale_x_continuous(breaks = seq(0, 1000, 100), labels = scales::comma_format()) +
  facet_wrap(coefficient ~ ., scales = "free_y", nrow = 3) + 
  theme(legend.position = "none") + 
  labs(title = "Backfitting Convergence", 
       x = "Iteration", 
       y = "Parameter Estimate")
```

(f)

```{r}
lm_coef <- lm(y ~ x1 + x2)$coefficients

data.frame(iteration = 1:1000, beta_0, beta_1, beta_2) %>%
  pivot_longer(-iteration, names_to = "coefficient", values_to = "estimate") %>%
  ggplot(aes(x = iteration, y = estimate, col = factor(coefficient))) + 
  geom_line() + 
  geom_hline(data = data.frame(yint = lm_coef[1], coefficient = "beta_0"), aes(yintercept = yint), linetype = "dashed") +
  geom_hline(data = data.frame(yint = lm_coef[2], coefficient = "beta_1"), aes(yintercept = yint), linetype = "dashed") +
  geom_hline(data = data.frame(yint = lm_coef[3], coefficient = "beta_2"), aes(yintercept = yint), linetype = "dashed") +
  scale_x_continuous(breaks = seq(0, 1000, 100), labels = scales::comma_format()) +
  facet_wrap(coefficient ~ ., scales = "free_y", nrow = 3) + 
  theme(legend.position = "none") + 
  labs(title = "Backfitting Convergence", 
       x = "Iteration", 
       y = "Parameter Estimate")
```
```{r}
data.frame(simple_reg = c(beta_0[1000], beta_1[1000], beta_2[1000]), multiple_reg = as.numeric(lm_coef))
```

The coefficients obtained from backfitting are exactly the same as the coefficients obtained from multiple linear regression.

(g)

```{r, warning=FALSE, message=FALSE}
data.frame(iteration = 1:1000, beta_0, beta_1, beta_2) %>%
  pivot_longer(-iteration, names_to = "coefficient", values_to = "estimate") %>%
  ggplot(aes(x = iteration, y = estimate, col = factor(coefficient))) + 
  geom_line() + 
  geom_hline(data = data.frame(yint = lm_coef[1], coefficient = "beta_0"), aes(yintercept = yint), linetype = "dashed") +
  geom_hline(data = data.frame(yint = lm_coef[2], coefficient = "beta_1"), aes(yintercept = yint), linetype = "dashed") +
  geom_hline(data = data.frame(yint = lm_coef[3], coefficient = "beta_2"), aes(yintercept = yint), linetype = "dashed") +
  geom_point() +
  scale_x_continuous(limits = c(1, 10), breaks = seq(1, 10), minor_breaks = NULL) +
  facet_wrap(coefficient ~ ., scales = "free_y", nrow = 3) + 
  theme(legend.position = "none") + 
  labs(title = "Backfitting Convergence", 
       x = "Iteration", 
       y = "Parameter Estimate")
```

It looks like convergence happens at iteration 3 from the graph, and that all subsequent iterations give the same estimates.

## Exercise 12
This problem is a continuation of the previous exercise. In a toy
example with $p = 100$, show that one can approximate the multiple
linear regression coefficient estimates by repeatedly performing simple
linear regression in a backfitting procedure. How many backfitting
iterations are required in order to obtain a “good” approximation to
the multiple regression coefficient estimates? Create a plot to justify
your answer

```{r}

p = 100
n = 1000

X = matrix(rnorm(n * p), ncol = p)
beta = rnorm(p)
Y = X %*% beta + rnorm(n)

beta_hat = rep(0, p)
num_iter = 5

# Stores the coefficients at each iteration
beta_hat_mat = matrix(0, nrow = num_iter, ncol = p)

for (i in 1:num_iter) {
  for (j in 1:p) {

    residual = Y - X[, -j] %*% beta_hat[-j]
    
    beta_hat[j] = lm(residual ~ X[, j])$coef[2]
  }
  
  beta_hat_mat[i, ] = beta_hat
}

# Sum of Squared Errors from the real vector of coefficients
error = apply(beta_hat_mat, 1, function(b) sum((b - beta)^2))

df = data.frame("SSE"=unlist(error),
                "iter"=1:length(error))

MLE_fit = lm(Y ~ X)
MLE_error = sum((MLE_fit$coefficients[-1] - beta)^2)

df %>% ggplot(aes(x=iter, y=SSE)) + geom_point() + geom_line() + 
  geom_hline(yintercept=MLE_error, col="indianred") + ylim(c(0, NA))
```
The algorithm converges after just a few iteration to the same value for $\hat{\beta}$ obtained via ML estimation, whose error is represented in the plot by the red line.


# GAM
This question is about using gam for univariate smoothing, the advantages of penalized regression and weighting a smooth model fit. The mcycle data in the MASS package are a classic dataset in univariate smoothing, introduced in Silverman (1985). The data measure the acceleration of the rider’s head, against time, in a simulated motorcycle crash.

1. Plot the acceleration against time, and use gam to fit a univariate smooth to the data, selecting the smoothing parameter by GCV (k of 30 to 40 is plenty for this example). Plot the resulting smooth, with partial residuals, but without standard errors.

2. Use lm and poly to fit a polynomial to the data, with approximately the same degrees of freedom as was estimated by gam. Use termplot to plot the estimated polynomial and partial residuals. Note the substantially worse fit achieved by the polynomial, relative to the penalized regression spline fit.

3. It’s possible to overstate the importance of penalization in explaining the improvement of the penalized regression spline, relative to the polynomial. Use gam to refit an un-penalized thin plate regression spline to the data, with basis dimension the same as that used for the polynomial, and again produce a plot for comparison with the previous two results.

4. Redo part 3 using an un-penalized cubic regression spline. You should find a fairly clear ordering of the acceptability of the results for the four models tried - what is it?

5. Now plot the model residuals against time, and comment.

6. Fit a linear model including a b-spline using the function bs on times and select a suitable degree and the knots position. Compare this model with the previous ones and comment.

**Solution**

```{r message=FALSE, warning=FALSE}
library(MASS)
library(mgcv)
data(mcycle)

# If we don't do this mcycle is a closure
df <- as.data.frame(matrix(c(mcycle$accel, mcycle$time), ncol = 2))
names(df) <- c("accel", "time")
```

```{r}
plot(df$time, df$accel)
```

1.
```{r}
RMSE <- rep(0, 12)

for (i in seq(30, 41)){
  gam_fit <- gam(accel ~ s(time, k = i), data = df)
  RMSE[i-29] <- mean(gam_fit$residuals^2)
}

plot(30:41, RMSE, type = "o", xlab = "k", ylab = "RMSE")
abline(v = 30 + which.min(RMSE) - 1, col = "red")
```

```{r}
fit_1 <- gam_fit <- gam(accel ~ s(time, k = 30+which.min(RMSE)-1), method = "GCV.Cp", data = df)
plot(fit_1, residuals = TRUE, se = FALSE)
```
```{r}
summary(fit_1)
```


2.
```{r}
fit_2 <- lm(accel ~ poly(time, round(summary(fit_1)$edf)), data = df)

termplot(fit_2, partial.resid = TRUE, se = FALSE)
```
```{r}
summary(fit_2)
```


We can notice that the polynomial fit is much worse than the penalized regression spline fit. This is because it can't manage to capture the linearity in the left part of the function.

3.
```{r}
fit_3 <- gam(accel ~ s(time, k = 12), method="GCV.Cp", sp=0, data = df)

plot(fit_3, residuals = TRUE, se = FALSE)
```
```{r}
summary(fit_3)
```
The fit is much better than the polynomial one and really similar to the penalized regression spline fit. Even if the penalization is not used, the fit results really smooth.

4.
```{r}
## Is bs="cc" the cubic spline?
fit_4 <- gam(accel ~ s(time, k = 12, bs="cr"), method = "GCV.Cp", sp=0, data=df)
plot(fit_4, residuals = TRUE, se = FALSE)
```
```{r}
summary(fit_4)
```

Even if the two fits are really similar, we have some differences on the right part of the function. The cubic spline fit is a little bit more smooth than the thin spline fit.

5.
```{r fig.height=10, fig.width=20}
par(mfrow=c(2,2))

plot(df$time, fit_1$residuals)
plot(df$time, fit_2$residuals)
plot(df$time, fit_3$residuals)
plot(df$time, fit_4$residuals)
```

We can notice that the residual plots looks all the same. They are all heteroskedastic because in the center we can notice more variance than in the left and right part of the function. Therefore in the left part we can notice some irregularities beside in the first fit, which looks more linear. From these plots we can notice that the first fit is the best one.

6.

We select the knots for each decile of the time variable and we add a knot in the right part of the function to capture the linearity.
```{r}
knots_1 <- c(quantile(df$time, probs = seq(0.1, 0.9, 0.1)), 55)
fit_5 <- lm(accel ~ bs(time, knots=knots_1), data = df, )
termplot(fit_5,  partial.resid = TRUE, se = FALSE)
abline(v = knots_1, col = "blue")
```

The curve looks really similar to the first fit. We can notice that the fit is not really smooth in the right part of the function. This is because we have only one knot in the right part of the function. If we add more knots in the right part of the function we can notice that the fit is more smooth.


```{r}
df2 <- data.frame("model" = seq(1, 5),
                  "dev_explained" = c(summary(fit_1)$dev.expl, summary(fit_2)$r.squared, summary(fit_3)$dev.expl, summary(fit_4)$dev.expl, summary(fit_5)$r.squared),
                  "AIC" = c(AIC(fit_1), AIC(fit_2), AIC(fit_3), AIC(fit_4), AIC(fit_5)),
                  "BIC" = c(BIC(fit_1), BIC(fit_2), BIC(fit_3), BIC(fit_4), BIC(fit_5)))
                  
```

```{r}
df2
```
From the table we can notice that the last fit is the best one w.r.t. the explained deviance but the AIC and the BIC are higher. Paying a little of explained deviance we can have a better fit with a lower AIC and BIC with the third fit. This differences are not really big, so we can choose one of all the fits based on the requirements of the problem. The only really bad fit is the second one, which has a really low explained deviance and a really high AIC and BIC.
